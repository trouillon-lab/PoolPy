{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=6\n",
    "dct_cmbn={}\n",
    "dct_cmbn.update({1:np.arange(N)})\n",
    "for j in range(2,5):\n",
    "    dct_cmbn.update({j:np.array(list(itertools.combinations(np.arange(N),j)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([0, 1, 2, 3, 4, 5]),\n",
       " 2: array([[0, 1],\n",
       "        [0, 2],\n",
       "        [0, 3],\n",
       "        [0, 4],\n",
       "        [0, 5],\n",
       "        [1, 2],\n",
       "        [1, 3],\n",
       "        [1, 4],\n",
       "        [1, 5],\n",
       "        [2, 3],\n",
       "        [2, 4],\n",
       "        [2, 5],\n",
       "        [3, 4],\n",
       "        [3, 5],\n",
       "        [4, 5]]),\n",
       " 3: array([[0, 1, 2],\n",
       "        [0, 1, 3],\n",
       "        [0, 1, 4],\n",
       "        [0, 1, 5],\n",
       "        [0, 2, 3],\n",
       "        [0, 2, 4],\n",
       "        [0, 2, 5],\n",
       "        [0, 3, 4],\n",
       "        [0, 3, 5],\n",
       "        [0, 4, 5],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 4],\n",
       "        [1, 2, 5],\n",
       "        [1, 3, 4],\n",
       "        [1, 3, 5],\n",
       "        [1, 4, 5],\n",
       "        [2, 3, 4],\n",
       "        [2, 3, 5],\n",
       "        [2, 4, 5],\n",
       "        [3, 4, 5]]),\n",
       " 4: array([[0, 1, 2, 3],\n",
       "        [0, 1, 2, 4],\n",
       "        [0, 1, 2, 5],\n",
       "        [0, 1, 3, 4],\n",
       "        [0, 1, 3, 5],\n",
       "        [0, 1, 4, 5],\n",
       "        [0, 2, 3, 4],\n",
       "        [0, 2, 3, 5],\n",
       "        [0, 2, 4, 5],\n",
       "        [0, 3, 4, 5],\n",
       "        [1, 2, 3, 4],\n",
       "        [1, 2, 3, 5],\n",
       "        [1, 2, 4, 5],\n",
       "        [1, 3, 4, 5],\n",
       "        [2, 3, 4, 5]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct_cmbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct_cmbn[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 6],\n",
       "       [1, 6],\n",
       "       [2, 6],\n",
       "       [3, 6],\n",
       "       [4, 6],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([dct_cmbn[1].T,np.array([6]*6)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 6],\n",
       "       [0, 2, 6],\n",
       "       [0, 3, 6],\n",
       "       [0, 4, 6],\n",
       "       [0, 5, 6],\n",
       "       [1, 2, 6],\n",
       "       [1, 3, 6],\n",
       "       [1, 4, 6],\n",
       "       [1, 5, 6],\n",
       "       [2, 3, 6],\n",
       "       [2, 4, 6],\n",
       "       [2, 5, 6],\n",
       "       [3, 4, 6],\n",
       "       [3, 5, 6],\n",
       "       [4, 5, 6],\n",
       "       [0, 1, 6],\n",
       "       [0, 2, 6],\n",
       "       [0, 3, 6],\n",
       "       [0, 4, 6],\n",
       "       [0, 5, 6],\n",
       "       [1, 2, 6],\n",
       "       [1, 3, 6],\n",
       "       [1, 4, 6],\n",
       "       [1, 5, 6],\n",
       "       [2, 3, 6],\n",
       "       [2, 4, 6],\n",
       "       [2, 5, 6],\n",
       "       [3, 4, 6],\n",
       "       [3, 5, 6],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([np.vstack([dct_cmbn[2].T,np.array([6]*len(dct_cmbn[2]))]).T,np.vstack([dct_cmbn[2].T,np.array([6]*len(dct_cmbn[2]))]).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 6],\n",
       "       [0, 2, 6],\n",
       "       [0, 3, 6],\n",
       "       [0, 4, 6],\n",
       "       [0, 5, 6],\n",
       "       [1, 2, 6],\n",
       "       [1, 3, 6],\n",
       "       [1, 4, 6],\n",
       "       [1, 5, 6],\n",
       "       [2, 3, 6],\n",
       "       [2, 4, 6],\n",
       "       [2, 5, 6],\n",
       "       [3, 4, 6],\n",
       "       [3, 5, 6],\n",
       "       [4, 5, 6],\n",
       "       [0, 1, 2],\n",
       "       [0, 1, 3],\n",
       "       [0, 1, 4],\n",
       "       [0, 1, 5],\n",
       "       [0, 2, 3],\n",
       "       [0, 2, 4],\n",
       "       [0, 2, 5],\n",
       "       [0, 3, 4],\n",
       "       [0, 3, 5],\n",
       "       [0, 4, 5],\n",
       "       [1, 2, 3],\n",
       "       [1, 2, 4],\n",
       "       [1, 2, 5],\n",
       "       [1, 3, 4],\n",
       "       [1, 3, 5],\n",
       "       [1, 4, 5],\n",
       "       [2, 3, 4],\n",
       "       [2, 3, 5],\n",
       "       [2, 4, 5],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([np.vstack([dct_cmbn[2].T,np.array([6]*len(dct_cmbn[2]))]),dct_cmbn[3].T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4],\n",
       "       [1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 4, 5, 5],\n",
       "       [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([dct_cmbn[2].T,np.array([6]*len(dct_cmbn[2]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_1(combinantions_dictionary, ND=5):\n",
    "    N=combinantions_dictionary[1][-1]+1\n",
    "    new_cd={1:np.append(combinantions_dictionary[1],N)}\n",
    "    diff=1\n",
    "    while diff<(ND-1):\n",
    "        new_part=np.vstack([combinantions_dictionary[diff].T,np.array([N]*len(combinantions_dictionary[diff]))])\n",
    "        new_in=np.hstack([combinantions_dictionary[diff+1].T,new_part]).T\n",
    "        new_cd.update({(diff+1):new_in})\n",
    "\n",
    "        diff+=1\n",
    "    \n",
    "\n",
    "    return(new_cd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=6\n",
    "dct_cmbn={}\n",
    "dct_cmbn.update({1:np.arange(N)})\n",
    "for j in range(2,5):\n",
    "    dct_cmbn.update({j:np.array(list(itertools.combinations(np.arange(N),j)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_dict={6:dct_cmbn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_N(full_dict,N_start, N_add):\n",
    "    tmp_d=full_dict[N_start]\n",
    "    i=0\n",
    "    while i<N_add:\n",
    "        tmp_d=add_1(tmp_d)\n",
    "        full_dict.update({(N_start+i+1):tmp_d})\n",
    "        i+=1\n",
    "\n",
    "    return(full_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_add_N(dict_start, N_add, save=True,save_dir='./combinations/'):\n",
    "    tmp_d=copy.deepcopy(dict_start)\n",
    "    N_start=dict_start[1][-1]\n",
    "    i=0\n",
    "    while i<N_add:\n",
    "        print(i)\n",
    "        tmp_d=add_1(tmp_d)\n",
    "        if save:\n",
    "            NM=save_dir+'N_'+str(N_start+i+1)+'.pk'\n",
    "            with open(NM, 'wb') as handle:\n",
    "                pickle.dump(tmp_d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        i+=1\n",
    "\n",
    "    return(tmp_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_consistent(well_assigner:np.array, differentiate:int) -> list:\n",
    "    if differentiate==0:\n",
    "        return(True,well_assigner, np.array([1]*well_assigner.shape[0]))\n",
    "    N=well_assigner.shape[0]\n",
    "    for i in range(differentiate):\n",
    "        diff=i+1\n",
    "        if diff ==1:\n",
    "            full_well_assigner=well_assigner.copy()\n",
    "        else:\n",
    "            N_cmbn=math.comb(N,diff)\n",
    "            temp_well_assigner=np.zeros((N_cmbn, well_assigner.shape[1]))==1\n",
    "            for l,k in enumerate(itertools.combinations(np.arange(N),diff)):\n",
    "                temp_well_assigner[l,:]=np.sum(well_assigner[k,:], axis=0).reshape(1,-1)\n",
    "            full_well_assigner=np.concatenate((full_well_assigner,temp_well_assigner))\n",
    "    _, counts=np.unique(full_well_assigner, axis=0, return_counts=True)\n",
    "    if np.unique(full_well_assigner, axis=0).shape[0]<full_well_assigner.shape[0]:\n",
    "        return(False, full_well_assigner, counts)\n",
    "    elif np.unique(full_well_assigner, axis=0).shape[0]==full_well_assigner.shape[0]:\n",
    "        return(True,full_well_assigner, counts)\n",
    "    else:\n",
    "        print(\"Something is fishy\")\n",
    "        return(-1)\n",
    "    \n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(well_assigner:np.ndarray, readout:np.ndarray, differentiate:int) -> list:\n",
    "    N=well_assigner.shape[0]\n",
    "    for i in range(differentiate):\n",
    "        resulti=[]\n",
    "        diff=i+1\n",
    "        if diff ==1:\n",
    "            resulti.extend(list(range(well_assigner.shape[0])))\n",
    "            full_well_assigner=well_assigner.copy()\n",
    "        else:\n",
    "            N_cmbn=math.comb(N,diff)\n",
    "            temp_well_assigner=np.zeros((N_cmbn, well_assigner.shape[1]))==1\n",
    "            for l,k in enumerate(itertools.combinations(np.arange(N),diff)):\n",
    "                resulti.append(k)\n",
    "                temp_well_assigner[l,:]=np.sum(well_assigner[k,:], axis=0).reshape(1,-1)\n",
    "            full_well_assigner=np.concatenate((full_well_assigner,temp_well_assigner))\n",
    "    idxs=[i for i in range(full_well_assigner.shape[0]) if np.array_equal(full_well_assigner[i,:],readout)]\n",
    "    if len(idxs)==0:\n",
    "        print('No match')\n",
    "        return(-1)\n",
    "    \n",
    "    return [resulti[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_method_comparison(**kwargs):\n",
    "    methods=['matrix', 'random', 'STD', 'Chinese trick']\n",
    "    # matrix assignment\n",
    "    \n",
    "\n",
    "    # multidimensional matrix\n",
    "    if 'n_dims' in kwargs.keys():\n",
    "        WA_mul=assign_wells_multidim(**kwargs)\n",
    "        ndmin=kwargs['n_dims']\n",
    "        multi=['multidim: '+str(ndmin)]\n",
    "        WA_list=[WA_mul]\n",
    "    elif 'all_dims' in kwargs.keys():\n",
    "        if kwargs['all_dims']:\n",
    "            WA_list=[]\n",
    "            multi=[]\n",
    "            for i in np.arange(2,int(np.ceil(np.log(kwargs['n_compounds'])/np.log(2)))):\n",
    "                if i>kwargs['max_dims']:\n",
    "                    continue\n",
    "                WA_mul=assign_wells_multidim(n_dims=i, **kwargs)\n",
    "                WA_list.append(WA_mul)\n",
    "                multi.append('multidim: '+str(i))\n",
    "\n",
    "        else:\n",
    "            ndmin= find_dims(**kwargs)\n",
    "            WA_mul=assign_wells_multidim(n_dims=ndmin, **kwargs)\n",
    "            multi=['multidim: '+str(ndmin)]\n",
    "            WA_list=[WA_mul]\n",
    "            \n",
    "\n",
    "\n",
    "    else:\n",
    "        ndmin= find_dims(**kwargs)\n",
    "        WA_mul=assign_wells_multidim(n_dims=ndmin, **kwargs)\n",
    "        multi=['multidim: '+str(ndmin)]\n",
    "        WA_list=[WA_mul]\n",
    "    \n",
    "    multi.extend(methods)\n",
    "    methods=multi.copy()\n",
    "\n",
    "    WA_mat=assign_wells_mat(**kwargs)\n",
    "\n",
    "    # random assignment\n",
    "\n",
    "    WA_ran=assign_wells_random(**kwargs)\n",
    "\n",
    "    # STD asignment \n",
    "    WA_std=assign_wells_STD(**kwargs)\n",
    "\n",
    "    \n",
    "\n",
    "    # chinese trick assignment\n",
    "    WA_chin=assign_wells_chinese(**kwargs)\n",
    "\n",
    "\n",
    "    WA_list.extend([WA_mat, WA_ran,WA_std, WA_chin])\n",
    "\n",
    "    if kwargs['differentiate']<2:\n",
    "        WA_bin=assign_wells_L(**kwargs)\n",
    "        methods.append('Binary')\n",
    "        WA_list.append(WA_bin)\n",
    "\n",
    "    #hierarchical\n",
    "        \n",
    "    Hier=calculate_metrics_hierarchical(**kwargs)\n",
    "    #return([BM[0], BM[1],layers, MC, details ])\n",
    "\n",
    "    ls_met=[]\n",
    "    ls_names_met=['mean_experiments', 'max_compounds_per_well', 'n_wells', 'percentage_check', 'mean_extra_exp', 'mean_steps']\n",
    "    for method, WA in zip(methods, WA_list):\n",
    "        mean_exp, extra_exp,  _, perc_check= mean_metrics(WA, **kwargs)\n",
    "        n_wells=WA.shape[1]\n",
    "        M_exp=np.round(mean_exp, 2)\n",
    "        max_comp=np.max(np.sum(WA, axis=0))\n",
    "        ls_met.append([M_exp, max_comp, n_wells, int(perc_check),  extra_exp,1+perc_check/100])\n",
    "    ls_met.append(Hier[:-1])\n",
    "    full_methods=methods.copy()\n",
    "    full_methods.append('Hierarchical')\n",
    "    df_met=pd.DataFrame(ls_met)\n",
    "\n",
    "    dict_wa={method: WA for method, WA in zip(methods, WA_list)}\n",
    "    dict_wa.update({'Hierarchical':Hier[5]})\n",
    "\n",
    "    idx_renamer={i:j for i,j in zip(df_met.index, full_methods)}\n",
    "    col_renamer={i:j for i,j in zip(df_met.columns, ls_names_met)}\n",
    "    df_met.rename(index=idx_renamer, columns=col_renamer, inplace=True)\n",
    "\n",
    "    ret_wa= kwargs['return_wa'] \n",
    "    if ret_wa:\n",
    "        return df_met, dict_wa\n",
    "    return df_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./combinations/N_294.pk', 'rb') as handle:\n",
    "    f1=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m F2\u001b[38;5;241m=\u001b[39m\u001b[43miterative_add_N\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 7\u001b[0m, in \u001b[0;36miterative_add_N\u001b[0;34m(dict_start, N_add, save, save_dir)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i\u001b[38;5;241m<\u001b[39mN_add:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 7\u001b[0m     tmp_d\u001b[38;5;241m=\u001b[39m\u001b[43madd_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save:\n\u001b[1;32m      9\u001b[0m         NM\u001b[38;5;241m=\u001b[39msave_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(N_start\u001b[38;5;241m+\u001b[39mi)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pk\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36madd_1\u001b[0;34m(combinantions_dictionary, ND)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m diff\u001b[38;5;241m<\u001b[39m(ND\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m     new_part\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mvstack([combinantions_dictionary[diff]\u001b[38;5;241m.\u001b[39mT,np\u001b[38;5;241m.\u001b[39marray([N]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(combinantions_dictionary[diff]))])\n\u001b[0;32m----> 7\u001b[0m     new_in\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcombinantions_dictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnew_part\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m      8\u001b[0m     new_cd\u001b[38;5;241m.\u001b[39mupdate({(diff\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):new_in})\n\u001b[1;32m     10\u001b[0m     diff\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/pooling_env/lib/python3.13/site-packages/numpy/_core/shape_base.py:364\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "F2=iterative_add_N(f1,400, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1=add_N(F_dict,6,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cmb_dict.pk', 'wb') as handle:\n",
    "    pickle.dump(F1, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1[1000][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(itertools.combinations(np.arange(1000),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri=add_1(dct_cmbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(itertools.combinations(np.arange(6),2))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_dir='./diff_1/20-47_step5.pk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_dir='./single_method/multidim_3/multidim_3__20-47_step5.pk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pk_dir, 'rb') as handle:\n",
    "    f1=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1[45]['WA'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To merge .pk files\n",
    "\n",
    "ls_names_met=['mean_experiments', 'max_compounds_per_well', 'n_wells', 'percentage_check', 'mean_extra_exp']\n",
    "full_dict={}\n",
    "for j in range(1,5):\n",
    "    base_dir='diff_'+str(j)\n",
    "    diff_dict={}\n",
    "    filenames = next(os.walk(base_dir), (None, None, []))[2]\n",
    "    for file in filenames:\n",
    "        full_dir=os.path.join(base_dir,file)\n",
    "        with open(full_dir, 'rb') as handle:\n",
    "            f1=pickle.load(handle)\n",
    "        for i in list(f1):\n",
    "            if i=='kwargs':\n",
    "                del f1['kwargs']\n",
    "                continue\n",
    "            else:\n",
    "                WA=assign_wells_mat(i)\n",
    "                f1[i][1].update({'matrix':WA})\n",
    "                mean_exp, extra_exp,  _, perc_check= mean_metrics(WA, differentiate=j)\n",
    "                n_wells=WA.shape[1]\n",
    "                M_exp=np.round(mean_exp, 2)\n",
    "                max_comp=np.max(np.sum(WA, axis=0))\n",
    "                ls_met=[M_exp, max_comp, n_wells, perc_check,  extra_exp]\n",
    "                f2=f1[i][0].drop(labels='matrix')\n",
    "                idxs=list(f2.index)\n",
    "                idxs+=['matrix']\n",
    "                r_idxs=idxs[-1:]+idxs[:-1]\n",
    "                dict_met={nm:list(f2[nm])+[val] for nm,val in zip(ls_names_met, ls_met)}\n",
    "\n",
    "                f1[i][0]=pd.DataFrame(data=dict_met, index=idxs).reindex(r_idxs)\n",
    "\n",
    "                \n",
    "                f1[i][0]=f1[i][0].round({'mean_extra_exp':2})\n",
    "            \n",
    "        diff_dict.update(f1)\n",
    "    full_str='Differentiate '+str(j)\n",
    "    full_dict.update({full_str:diff_dict})\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output as pickle file\n",
    "full_dir='Final_precomputed_file.pk'\n",
    "\n",
    "#with open(full_dir, 'wb') as handle:\n",
    "    #pickle.dump(full_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To merge files generated with inline_print\n",
    "all_methods = ['matrix', 'multidim', 'random', 'STD', 'CT', 'Binary']\n",
    "results = {}\n",
    "\n",
    "for method_dir in all_methods:\n",
    "    filenames = next(os.walk(method_dir), (None, None, []))[2]\n",
    "    metrics_list = []\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".txt\"):\n",
    "            try:\n",
    "                metrics = extract_metrics(file)\n",
    "                metrics[\"method\"] = method_dir\n",
    "                metrics_list.append(metrics)\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "    \n",
    "    if metrics_list:\n",
    "        df = pd.DataFrame(metrics_list)\n",
    "        df = df.sort_values(by=\"NS\")\n",
    "        \n",
    "        # Group by diff value\n",
    "        for diff_value, group_df in df.groupby(\"diff\"):\n",
    "            group_df = group_df.reset_index(drop=True)\n",
    "            \n",
    "            if diff_value not in results:\n",
    "                results['Differentiate ' + str(diff_value)] = {}\n",
    "                \n",
    "            for ns_value, ns_group_df in group_df.groupby(\"NS\"):\n",
    "                ns_group_df = ns_group_df.drop(columns=[\"diff\", \"NS\"]) \n",
    "                ns_group_df = ns_group_df.rename(columns={\"NW\": \"n_wells\", \"MS\": \"max_compounds_per_well\"})\n",
    "               \n",
    "                ns_group_df = ns_group_df.set_index([\"method\"])\n",
    "                \n",
    "                if ns_value not in results['Differentiate ' + str(diff_value)]:\n",
    "                    results['Differentiate ' + str(diff_value)][ns_value] = [ns_group_df, {}]\n",
    "                else:\n",
    "                    existing_df, empty_dict = results['Differentiate ' + str(diff_value)][ns_value]\n",
    "                    \n",
    "                    merged_df = pd.merge(existing_df.reset_index(), ns_group_df.reset_index(),\n",
    "                                         on=[\"method\", \"n_wells\", \"max_compounds_per_well\"], how=\"outer\", indicator=True)\n",
    "                    \n",
    "                    # Filter out rows that are duplicates (the '_merge' column indicates if it's from both dataframes)\n",
    "                    new_entries_df = merged_df[merged_df['_merge'] == 'right_only'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    existing_entries_df = merged_df[merged_df['_merge'] == 'both'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    \n",
    "                    # Concatenate only new entries\n",
    "                    combined_df = pd.concat([existing_entries_df, new_entries_df]).drop_duplicates().reset_index(drop=True)\n",
    "                    \n",
    "                    # Reassign the combined dataframe\n",
    "                    results['Differentiate ' + str(diff_value)][ns_value] = [combined_df, {}]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Output as pickle file\n",
    "full_dir = 'Final_inline_precomputed_file.pk'\n",
    "\n",
    "# First check if file already exists\n",
    "# Merge new and old if it does\n",
    "if os.path.exists(full_dir):\n",
    "    with open(full_dir, \"rb\") as f:\n",
    "        existing_results = pickle.load(f)\n",
    "        \n",
    "    for diff_value, ns_values in results.items():\n",
    "        if diff_value not in existing_results:\n",
    "            existing_df, existing_empty_dict = existing_results[diff_value][ns_value]\n",
    "            \n",
    "            for ns_value, (new_df, empty_dict) in ns_values.items():\n",
    "                if ns_value in existing_results[diff_value]:\n",
    "                    existing_df, existing_empty_dict = existing_results[diff_value][ns_value]\n",
    "                \n",
    "            \n",
    "                    merged_df = pd.merge(existing_df.reset_index(), new_df.reset_index(),\n",
    "                                         on=[\"method\", \"n_wells\", \"max_compounds_per_well\"], how=\"outer\", indicator=True)\n",
    "                    \n",
    "                    new_entries_df = merged_df[merged_df['_merge'] == 'right_only'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    existing_entries_df = merged_df[merged_df['_merge'] == 'both'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    \n",
    "                    combined_df = pd.concat([existing_entries_df, new_entries_df]).drop_duplicates().reset_index(drop=True)\n",
    "                    existing_results[diff_value][ns_value] = [combined_df.set_index([\"method\"]), {}]\n",
    "                else:\n",
    "                    existing_results[diff_value][ns_value] = [new_df, empty_dict]\n",
    "            \n",
    "                \n",
    "    merged_results = existing_results\n",
    "    \n",
    "else:\n",
    "    merged_results = results\n",
    "\n",
    "#with open(full_dir, 'wb') as handle:\n",
    "    #pickle.dump(merged_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pooling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
