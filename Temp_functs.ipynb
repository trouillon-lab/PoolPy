{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=6\n",
    "dct_cmbn={}\n",
    "dct_cmbn.update({1:np.arange(N)})\n",
    "for j in range(2,5):\n",
    "    dct_cmbn.update({j:np.array(list(itertools.combinations(np.arange(N),j)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([0, 1, 2, 3, 4, 5]),\n",
       " 2: array([[0, 1],\n",
       "        [0, 2],\n",
       "        [0, 3],\n",
       "        [0, 4],\n",
       "        [0, 5],\n",
       "        [1, 2],\n",
       "        [1, 3],\n",
       "        [1, 4],\n",
       "        [1, 5],\n",
       "        [2, 3],\n",
       "        [2, 4],\n",
       "        [2, 5],\n",
       "        [3, 4],\n",
       "        [3, 5],\n",
       "        [4, 5]]),\n",
       " 3: array([[0, 1, 2],\n",
       "        [0, 1, 3],\n",
       "        [0, 1, 4],\n",
       "        [0, 1, 5],\n",
       "        [0, 2, 3],\n",
       "        [0, 2, 4],\n",
       "        [0, 2, 5],\n",
       "        [0, 3, 4],\n",
       "        [0, 3, 5],\n",
       "        [0, 4, 5],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 4],\n",
       "        [1, 2, 5],\n",
       "        [1, 3, 4],\n",
       "        [1, 3, 5],\n",
       "        [1, 4, 5],\n",
       "        [2, 3, 4],\n",
       "        [2, 3, 5],\n",
       "        [2, 4, 5],\n",
       "        [3, 4, 5]]),\n",
       " 4: array([[0, 1, 2, 3],\n",
       "        [0, 1, 2, 4],\n",
       "        [0, 1, 2, 5],\n",
       "        [0, 1, 3, 4],\n",
       "        [0, 1, 3, 5],\n",
       "        [0, 1, 4, 5],\n",
       "        [0, 2, 3, 4],\n",
       "        [0, 2, 3, 5],\n",
       "        [0, 2, 4, 5],\n",
       "        [0, 3, 4, 5],\n",
       "        [1, 2, 3, 4],\n",
       "        [1, 2, 3, 5],\n",
       "        [1, 2, 4, 5],\n",
       "        [1, 3, 4, 5],\n",
       "        [2, 3, 4, 5]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct_cmbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct_cmbn[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 6],\n",
       "       [1, 6],\n",
       "       [2, 6],\n",
       "       [3, 6],\n",
       "       [4, 6],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([dct_cmbn[1].T,np.array([6]*6)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 6],\n",
       "       [0, 2, 6],\n",
       "       [0, 3, 6],\n",
       "       [0, 4, 6],\n",
       "       [0, 5, 6],\n",
       "       [1, 2, 6],\n",
       "       [1, 3, 6],\n",
       "       [1, 4, 6],\n",
       "       [1, 5, 6],\n",
       "       [2, 3, 6],\n",
       "       [2, 4, 6],\n",
       "       [2, 5, 6],\n",
       "       [3, 4, 6],\n",
       "       [3, 5, 6],\n",
       "       [4, 5, 6],\n",
       "       [0, 1, 6],\n",
       "       [0, 2, 6],\n",
       "       [0, 3, 6],\n",
       "       [0, 4, 6],\n",
       "       [0, 5, 6],\n",
       "       [1, 2, 6],\n",
       "       [1, 3, 6],\n",
       "       [1, 4, 6],\n",
       "       [1, 5, 6],\n",
       "       [2, 3, 6],\n",
       "       [2, 4, 6],\n",
       "       [2, 5, 6],\n",
       "       [3, 4, 6],\n",
       "       [3, 5, 6],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([np.vstack([dct_cmbn[2].T,np.array([6]*len(dct_cmbn[2]))]).T,np.vstack([dct_cmbn[2].T,np.array([6]*len(dct_cmbn[2]))]).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 6],\n",
       "       [0, 2, 6],\n",
       "       [0, 3, 6],\n",
       "       [0, 4, 6],\n",
       "       [0, 5, 6],\n",
       "       [1, 2, 6],\n",
       "       [1, 3, 6],\n",
       "       [1, 4, 6],\n",
       "       [1, 5, 6],\n",
       "       [2, 3, 6],\n",
       "       [2, 4, 6],\n",
       "       [2, 5, 6],\n",
       "       [3, 4, 6],\n",
       "       [3, 5, 6],\n",
       "       [4, 5, 6],\n",
       "       [0, 1, 2],\n",
       "       [0, 1, 3],\n",
       "       [0, 1, 4],\n",
       "       [0, 1, 5],\n",
       "       [0, 2, 3],\n",
       "       [0, 2, 4],\n",
       "       [0, 2, 5],\n",
       "       [0, 3, 4],\n",
       "       [0, 3, 5],\n",
       "       [0, 4, 5],\n",
       "       [1, 2, 3],\n",
       "       [1, 2, 4],\n",
       "       [1, 2, 5],\n",
       "       [1, 3, 4],\n",
       "       [1, 3, 5],\n",
       "       [1, 4, 5],\n",
       "       [2, 3, 4],\n",
       "       [2, 3, 5],\n",
       "       [2, 4, 5],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([np.vstack([dct_cmbn[2].T,np.array([6]*len(dct_cmbn[2]))]),dct_cmbn[3].T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4],\n",
       "       [1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 4, 5, 5],\n",
       "       [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([dct_cmbn[2].T,np.array([6]*len(dct_cmbn[2]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_1(combinantions_dictionary, ND=5):\n",
    "    N=combinantions_dictionary[1][-1]+1\n",
    "    new_cd={1:np.append(combinantions_dictionary[1],N)}\n",
    "    diff=1\n",
    "    while diff<(ND-1):\n",
    "        new_part=np.vstack([combinantions_dictionary[diff].T,np.array([N]*len(combinantions_dictionary[diff]))])\n",
    "        new_in=np.hstack([combinantions_dictionary[diff+1].T,new_part]).T\n",
    "        new_cd.update({(diff+1):new_in})\n",
    "\n",
    "        diff+=1\n",
    "    \n",
    "\n",
    "    return(new_cd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=6\n",
    "dct_cmbn={}\n",
    "dct_cmbn.update({1:np.arange(N)})\n",
    "for j in range(2,5):\n",
    "    dct_cmbn.update({j:np.array(list(itertools.combinations(np.arange(N),j)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_dict={6:dct_cmbn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_N(full_dict,N_start, N_add):\n",
    "    tmp_d=full_dict[N_start]\n",
    "    i=0\n",
    "    while i<N_add:\n",
    "        tmp_d=add_1(tmp_d)\n",
    "        full_dict.update({(N_start+i+1):tmp_d})\n",
    "        i+=1\n",
    "\n",
    "    return(full_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_add_N(dict_start,N_start, N_add, save=True,save_dir='./combinations/'):\n",
    "    tmp_d=copy.deepcopy(dict_start)\n",
    "    i=0\n",
    "    while i<N_add:\n",
    "        tmp_d=add_1(tmp_d)\n",
    "        if save:\n",
    "            NM=save_dir+'N_'+N_start+i+'.pk'\n",
    "            with open(NM, 'wb') as handle:\n",
    "                pickle.dump(tmp_d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        i+=1\n",
    "\n",
    "    return(tmp_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F2=iterative_add_N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1=add_N(F_dict,6,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cmb_dict.pk', 'wb') as handle:\n",
    "    pickle.dump(F1, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1[1000][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(itertools.combinations(np.arange(1000),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri=add_1(dct_cmbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(itertools.combinations(np.arange(6),2))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_dir='./diff_1/20-47_step5.pk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_dir='./single_method/multidim_3/multidim_3__20-47_step5.pk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pk_dir, 'rb') as handle:\n",
    "    f1=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1[45]['WA'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To merge .pk files\n",
    "\n",
    "ls_names_met=['mean_experiments', 'max_compounds_per_well', 'n_wells', 'percentage_check', 'mean_extra_exp']\n",
    "full_dict={}\n",
    "for j in range(1,5):\n",
    "    base_dir='diff_'+str(j)\n",
    "    diff_dict={}\n",
    "    filenames = next(os.walk(base_dir), (None, None, []))[2]\n",
    "    for file in filenames:\n",
    "        full_dir=os.path.join(base_dir,file)\n",
    "        with open(full_dir, 'rb') as handle:\n",
    "            f1=pickle.load(handle)\n",
    "        for i in list(f1):\n",
    "            if i=='kwargs':\n",
    "                del f1['kwargs']\n",
    "                continue\n",
    "            else:\n",
    "                WA=assign_wells_mat(i)\n",
    "                f1[i][1].update({'matrix':WA})\n",
    "                mean_exp, extra_exp,  _, perc_check= mean_metrics(WA, differentiate=j)\n",
    "                n_wells=WA.shape[1]\n",
    "                M_exp=np.round(mean_exp, 2)\n",
    "                max_comp=np.max(np.sum(WA, axis=0))\n",
    "                ls_met=[M_exp, max_comp, n_wells, perc_check,  extra_exp]\n",
    "                f2=f1[i][0].drop(labels='matrix')\n",
    "                idxs=list(f2.index)\n",
    "                idxs+=['matrix']\n",
    "                r_idxs=idxs[-1:]+idxs[:-1]\n",
    "                dict_met={nm:list(f2[nm])+[val] for nm,val in zip(ls_names_met, ls_met)}\n",
    "\n",
    "                f1[i][0]=pd.DataFrame(data=dict_met, index=idxs).reindex(r_idxs)\n",
    "\n",
    "                \n",
    "                f1[i][0]=f1[i][0].round({'mean_extra_exp':2})\n",
    "            \n",
    "        diff_dict.update(f1)\n",
    "    full_str='Differentiate '+str(j)\n",
    "    full_dict.update({full_str:diff_dict})\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output as pickle file\n",
    "full_dir='Final_precomputed_file.pk'\n",
    "\n",
    "#with open(full_dir, 'wb') as handle:\n",
    "    #pickle.dump(full_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To merge files generated with inline_print\n",
    "all_methods = ['matrix', 'multidim', 'random', 'STD', 'CT', 'Binary']\n",
    "results = {}\n",
    "\n",
    "for method_dir in all_methods:\n",
    "    filenames = next(os.walk(method_dir), (None, None, []))[2]\n",
    "    metrics_list = []\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".txt\"):\n",
    "            try:\n",
    "                metrics = extract_metrics(file)\n",
    "                metrics[\"method\"] = method_dir\n",
    "                metrics_list.append(metrics)\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "    \n",
    "    if metrics_list:\n",
    "        df = pd.DataFrame(metrics_list)\n",
    "        df = df.sort_values(by=\"NS\")\n",
    "        \n",
    "        # Group by diff value\n",
    "        for diff_value, group_df in df.groupby(\"diff\"):\n",
    "            group_df = group_df.reset_index(drop=True)\n",
    "            \n",
    "            if diff_value not in results:\n",
    "                results['Differentiate ' + str(diff_value)] = {}\n",
    "                \n",
    "            for ns_value, ns_group_df in group_df.groupby(\"NS\"):\n",
    "                ns_group_df = ns_group_df.drop(columns=[\"diff\", \"NS\"]) \n",
    "                ns_group_df = ns_group_df.rename(columns={\"NW\": \"n_wells\", \"MS\": \"max_compounds_per_well\"})\n",
    "               \n",
    "                ns_group_df = ns_group_df.set_index([\"method\"])\n",
    "                \n",
    "                if ns_value not in results['Differentiate ' + str(diff_value)]:\n",
    "                    results['Differentiate ' + str(diff_value)][ns_value] = [ns_group_df, {}]\n",
    "                else:\n",
    "                    existing_df, empty_dict = results['Differentiate ' + str(diff_value)][ns_value]\n",
    "                    \n",
    "                    merged_df = pd.merge(existing_df.reset_index(), ns_group_df.reset_index(),\n",
    "                                         on=[\"method\", \"n_wells\", \"max_compounds_per_well\"], how=\"outer\", indicator=True)\n",
    "                    \n",
    "                    # Filter out rows that are duplicates (the '_merge' column indicates if it's from both dataframes)\n",
    "                    new_entries_df = merged_df[merged_df['_merge'] == 'right_only'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    existing_entries_df = merged_df[merged_df['_merge'] == 'both'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    \n",
    "                    # Concatenate only new entries\n",
    "                    combined_df = pd.concat([existing_entries_df, new_entries_df]).drop_duplicates().reset_index(drop=True)\n",
    "                    \n",
    "                    # Reassign the combined dataframe\n",
    "                    results['Differentiate ' + str(diff_value)][ns_value] = [combined_df, {}]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Output as pickle file\n",
    "full_dir = 'Final_inline_precomputed_file.pk'\n",
    "\n",
    "# First check if file already exists\n",
    "# Merge new and old if it does\n",
    "if os.path.exists(full_dir):\n",
    "    with open(full_dir, \"rb\") as f:\n",
    "        existing_results = pickle.load(f)\n",
    "        \n",
    "    for diff_value, ns_values in results.items():\n",
    "        if diff_value not in existing_results:\n",
    "            existing_df, existing_empty_dict = existing_results[diff_value][ns_value]\n",
    "            \n",
    "            for ns_value, (new_df, empty_dict) in ns_values.items():\n",
    "                if ns_value in existing_results[diff_value]:\n",
    "                    existing_df, existing_empty_dict = existing_results[diff_value][ns_value]\n",
    "                \n",
    "            \n",
    "                    merged_df = pd.merge(existing_df.reset_index(), new_df.reset_index(),\n",
    "                                         on=[\"method\", \"n_wells\", \"max_compounds_per_well\"], how=\"outer\", indicator=True)\n",
    "                    \n",
    "                    new_entries_df = merged_df[merged_df['_merge'] == 'right_only'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    existing_entries_df = merged_df[merged_df['_merge'] == 'both'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    \n",
    "                    combined_df = pd.concat([existing_entries_df, new_entries_df]).drop_duplicates().reset_index(drop=True)\n",
    "                    existing_results[diff_value][ns_value] = [combined_df.set_index([\"method\"]), {}]\n",
    "                else:\n",
    "                    existing_results[diff_value][ns_value] = [new_df, empty_dict]\n",
    "            \n",
    "                \n",
    "    merged_results = existing_results\n",
    "    \n",
    "else:\n",
    "    merged_results = results\n",
    "\n",
    "#with open(full_dir, 'wb') as handle:\n",
    "    #pickle.dump(merged_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pooling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
