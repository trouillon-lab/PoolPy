{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=6\n",
    "dct_cmbn={}\n",
    "dct_cmbn.update({1:np.arange(N)})\n",
    "for j in range(2,5):\n",
    "    dct_cmbn.update({j:np.array(list(itertools.combinations(np.arange(N),j)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_1(combinantions_dictionary, ND=5):\n",
    "    N=combinantions_dictionary[1][-1]+1\n",
    "    new_cd={1:np.append(combinantions_dictionary[1],N)}\n",
    "    diff=1\n",
    "    while diff<(ND):\n",
    "        new_part=np.vstack([combinantions_dictionary[diff].T,np.array([N]*len(combinantions_dictionary[diff]))])\n",
    "        new_in=np.hstack([combinantions_dictionary[diff+1].T,new_part]).T\n",
    "        new_cd.update({(diff+1):new_in})\n",
    "\n",
    "        diff+=1\n",
    "    \n",
    "\n",
    "    return(new_cd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=34\n",
    "dct_cmbn={}\n",
    "dct_cmbn.update({1:np.arange(N)})\n",
    "for j in range(2,6):\n",
    "    dct_cmbn.update({j:np.array(list(itertools.combinations(np.arange(N),j)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_dict={6:dct_cmbn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_N(full_dict,N_start, N_add):\n",
    "    tmp_d=full_dict[N_start]\n",
    "    i=0\n",
    "    while i<N_add:\n",
    "        tmp_d=add_1(tmp_d)\n",
    "        full_dict.update({(N_start+i+1):tmp_d})\n",
    "        i+=1\n",
    "\n",
    "    return(full_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_add_N(dict_start, N_add, save=True,save_dir='./combinations/'):\n",
    "    tmp_d=copy.deepcopy(dict_start)\n",
    "    N_start=dict_start[1][-1]\n",
    "    i=0\n",
    "    while i<N_add:\n",
    "        print(i)\n",
    "        tmp_d=add_1(tmp_d)\n",
    "        if save:\n",
    "            NM=save_dir+'N_'+str(N_start+i+1)+'.pk'\n",
    "            with open(NM, 'wb') as handle:\n",
    "                pickle.dump(tmp_d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        i+=1\n",
    "\n",
    "    return(tmp_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "WA_mat=assign_wells_mat(n_compounds=35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 12)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WA_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46376, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct_cmbn[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=35\n",
    "diff=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_cmbn={}\n",
    "dct_cmbn.update({1:np.arange(N)})\n",
    "for j in range(2,diff+1):\n",
    "    dct_cmbn.update({j:np.array(list(itertools.combinations(np.arange(N),j)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=add_1(dct_cmbn, ND=diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58905, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dct_cmbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52360, 4, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WA_mat[f1[4]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm=np.bool(np.sum(WA_mat[f1[4]], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False, False, False,  True,  True,  True,\n",
       "        True, False, False])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52360, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#NMO=save_dir+'N_'+str(N)+'.pk'\n",
    "NMO='./combinations/N_294.pk'\n",
    "with open(NMO, \"rb\") as input_file:\n",
    "    scrambler = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "        260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "        273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "        286, 287, 288, 289, 290, 291, 292, 293, 294, 295]),\n",
       " 2: array([[  0,   1],\n",
       "        [  0,   2],\n",
       "        [  0,   3],\n",
       "        ...,\n",
       "        [292, 295],\n",
       "        [293, 295],\n",
       "        [294, 295]]),\n",
       " 3: array([[  0,   1,   2],\n",
       "        [  0,   1,   3],\n",
       "        [  0,   1,   4],\n",
       "        ...,\n",
       "        [291, 294, 295],\n",
       "        [292, 294, 295],\n",
       "        [293, 294, 295]]),\n",
       " 4: array([[  0,   1,   2,   3],\n",
       "        [  0,   1,   2,   4],\n",
       "        [  0,   1,   2,   5],\n",
       "        ...,\n",
       "        [290, 293, 294, 295],\n",
       "        [291, 293, 294, 295],\n",
       "        [292, 293, 294, 295]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrambler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_consistent_precomp(well_assigner:np.array, differentiate:int, scrambler:dict) -> list:\n",
    "    if differentiate==0:\n",
    "        return(True,well_assigner, np.array([1]*well_assigner.shape[0]))\n",
    "    N=well_assigner.shape[0]\n",
    "    for i in range(differentiate):\n",
    "        diff=i+1\n",
    "        if diff ==1:\n",
    "            full_well_assigner=well_assigner.copy()\n",
    "        else:\n",
    "            this_sc=scrambler[diff]\n",
    "            full_well_assigner=np.concatenate((full_well_assigner,np.bool(np.sum(well_assigner[this_sc], axis=1))))\n",
    "    _, counts=np.unique(full_well_assigner, axis=0, return_counts=True)\n",
    "    if len(counts)<full_well_assigner.shape[0]:\n",
    "        return(False, full_well_assigner, counts)\n",
    "    elif len(counts)==full_well_assigner.shape[0]:\n",
    "        return(True,full_well_assigner, counts)\n",
    "    else:\n",
    "        print(\"Something is fishy\")\n",
    "        return(-1)\n",
    "    \n",
    "#\n",
    "def mean_metrics_precomp(well_assigner, differentiate, scrambler, **kwargs):\n",
    "    BT=well_assigner.shape[1]\n",
    "    _,_, counts= is_consistent_precomp(well_assigner, differentiate, scrambler) \n",
    "    ET=extra_tests(counts)   \n",
    "    rounds=np.sum(counts>1)/np.sum(counts>0)+1\n",
    "    p_check=np.round(np.sum(counts[counts>1])/np.sum(counts)*100)\n",
    "    return BT+ET, ET,  rounds, p_check\n",
    "\n",
    "def decode_precomp(well_assigner:np.ndarray, readout:np.ndarray, differentiate:int, scrambler:dict) -> list:\n",
    "    N=well_assigner.shape[0]\n",
    "    for i in range(differentiate):\n",
    "        resulti=[]\n",
    "        diff=i+1\n",
    "        if diff ==1:\n",
    "            resulti.extend(list(range(well_assigner.shape[0])))\n",
    "            full_well_assigner=well_assigner.copy()\n",
    "        else:\n",
    "            this_sc=scrambler[diff]\n",
    "            full_well_assigner=np.concatenate((full_well_assigner,np.bool(np.sum(well_assigner[this_sc], axis=1))))\n",
    "    idxs=[i for i in range(full_well_assigner.shape[0]) if np.array_equal(full_well_assigner[i,:],readout)]\n",
    "    if len(idxs)==0:\n",
    "        print('No match')\n",
    "        return(-1)\n",
    "    \n",
    "    return [resulti[i] for i in idxs]\n",
    "\n",
    "def find_rand_params_precomp(n_compounds:int, differentiate:int, scrambler:dict, n_compounds_per_well=0, n_wells=0, guesses=0, \n",
    "                     max_compounds=0, max_redundancy=4, min_redundancy=1):\n",
    "    skip_compounds=True\n",
    "    skip_wells=True\n",
    "    if n_compounds_per_well==0:\n",
    "        skip_compounds=False\n",
    "    if n_wells==0:\n",
    "        skip_wells=False\n",
    "    if guesses==0:\n",
    "        guesses=n_compounds\n",
    "\n",
    "    MC= int(n_compounds/2) if max_compounds==0 else max_compounds\n",
    "    mc=int(np.sqrt(n_compounds)) if int(np.sqrt(n_compounds))<MC else int(MC/2)\n",
    "    arr_comp=np.arange(int(mc),int(MC+1))\n",
    "    mw=int(np.log2(n_compounds))\n",
    "    MW=int(2*np.sqrt(n_compounds))\n",
    "    while MW-mw<10:\n",
    "        mw=int(abs(mw-1))\n",
    "        MW=int(MW+1)\n",
    "\n",
    "    arr_wells=np.arange(mw,MW)\n",
    "    min_tests=np.inf\n",
    "    for comp in arr_comp:\n",
    "        if skip_compounds:\n",
    "            comp=n_compounds_per_well\n",
    "\n",
    "        for wells in arr_wells:\n",
    "            if skip_wells:\n",
    "                if skip_compounds:\n",
    "                    \n",
    "                    return n_compounds_per_well, n_wells, assign_wells_random_precomp(n_compounds=n_compounds, differentiate=differentiate, \n",
    "                                               n_compounds_per_well=n_compounds_per_well, n_wells=n_wells, guesses=guesses, Evaluate=True, \n",
    "                                               scrambler=scrambler)\n",
    "                wells=n_wells\n",
    "                \n",
    "            if comp*wells>max_redundancy*n_compounds*np.log2(n_compounds) or comp*wells<min_redundancy*n_compounds: continue \n",
    "            WA_tmp, mean_exp=assign_wells_random_precomp(n_compounds, differentiate, comp, wells,\n",
    "                                                          guesses, Evaluate=True, return_me=True,scrambler=scrambler)\n",
    "            if mean_exp<min_tests:\n",
    "                Comp=comp\n",
    "                Wells=wells\n",
    "                min_tests=mean_exp\n",
    "                min_wa=WA_tmp\n",
    "            if skip_wells:\n",
    "                break\n",
    "        if skip_compounds:\n",
    "            break\n",
    "\n",
    "    return Comp, Wells, min_tests, min_wa\n",
    "\n",
    "def assign_wells_random_precomp(n_compounds:int,  differentiate:int,scrambler:dict, n_compounds_per_well=0, \n",
    "                        n_wells=0, guesses=0, Evaluate=False, return_me=False, **kwargs)->np.array:\n",
    "    if guesses==0:\n",
    "        guesses=n_compounds\n",
    "    min_tests=np.inf\n",
    "\n",
    "    if n_compounds_per_well==0 or n_wells==0:\n",
    "        _,_, min_tests, WA_rand=find_rand_params_precomp(n_compounds=n_compounds, differentiate=differentiate, \n",
    "                                 n_compounds_per_well=n_compounds_per_well, n_wells=n_wells, guesses=guesses,\n",
    "                                 scrambler=scrambler)\n",
    "        if return_me:\n",
    "            return WA_rand,  min_tests\n",
    "        \n",
    "        return WA_rand\n",
    "        \n",
    "\n",
    "\n",
    "    if Evaluate:\n",
    "        second_axis=np.tile(np.arange(n_wells),n_compounds_per_well).reshape(n_compounds_per_well,-1)\n",
    "        for i in range(guesses):\n",
    "            idt=np.random.randint(0,n_compounds,size=(n_compounds_per_well,n_wells) )\n",
    "            well_assigner=np.zeros((n_compounds,n_wells))==1\n",
    "            well_assigner[idt, second_axis]=True\n",
    "            if guesses==1:\n",
    "                if return_me:\n",
    "                    mean_exp, _, _, p_check= mean_metrics_precomp(well_assigner=well_assigner, \n",
    "                                                                differentiate=differentiate,scrambler=scrambler)\n",
    "                    return well_assigner, mean_exp\n",
    "                return well_assigner\n",
    "            mean_exp, _, _, p_check= mean_metrics_precomp(well_assigner=well_assigner,\n",
    "                                                        differentiate=differentiate, scrambler=scrambler)\n",
    "            if p_check<1:\n",
    "                if return_me:\n",
    "                    return well_assigner,  mean_exp\n",
    "                return well_assigner\n",
    "            elif mean_exp<min_tests: \n",
    "                best_wa=well_assigner.copy()\n",
    "                min_tests=mean_exp\n",
    "\n",
    "        if return_me:\n",
    "            return best_wa,  min_tests\n",
    "        \n",
    "        return best_wa\n",
    "\n",
    "    _,_, min_tests, WA_rand=find_rand_params_precomp(n_compounds=n_compounds, differentiate=differentiate, \n",
    "                                 n_compounds_per_well=n_compounds_per_well, n_wells=n_wells, \n",
    "                                 guesses=guesses,scrambler=scrambler)\n",
    "    if return_me:\n",
    "        return WA_rand,  min_tests\n",
    "    \n",
    "    return WA_rand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_sweep_deterministic_WAs(start=50, stop=150, step=10, **kwargs):\n",
    "    dict_comp={}\n",
    "    current=start\n",
    "    kwargs['return_wa']=True\n",
    "    while current<stop:\n",
    "        time0=time.time()\n",
    "        if kwargs['timeit']:\n",
    "            print(current)\n",
    "        df_met, dict_wa=full_deterministic_WAS(n_compounds=current, **kwargs)\n",
    "        dict_comp.update({current:[df_met, dict_wa]})\n",
    "        current=current+step\n",
    "        if kwargs['timeit']:\n",
    "            print(\"segment time: %s seconds\" % np.round(time.time() - time0, 1))\n",
    "    return dict_comp\n",
    "\n",
    "\n",
    "def full_deterministic_WAS(**kwargs):\n",
    "    methods=['matrix', 'random', 'STD', 'Chinese trick']\n",
    "    # matrix assignment\n",
    "    \n",
    "\n",
    "    # multidimensional matrix\n",
    "    if 'n_dims' in kwargs.keys():\n",
    "        WA_mul=assign_wells_multidim(**kwargs)\n",
    "        ndmin=kwargs['n_dims']\n",
    "        multi=['multidim: '+str(ndmin)]\n",
    "        WA_list=[WA_mul]\n",
    "    elif 'all_dims' in kwargs.keys():\n",
    "        if kwargs['all_dims']:\n",
    "            WA_list=[]\n",
    "            multi=[]\n",
    "            for i in np.arange(2,int(np.ceil(np.log(kwargs['n_compounds'])/np.log(2)))):\n",
    "                if i>kwargs['max_dims']:\n",
    "                    continue\n",
    "                WA_mul=assign_wells_multidim(n_dims=i, **kwargs)\n",
    "                WA_list.append(WA_mul)\n",
    "                multi.append('multidim_'+str(i))\n",
    "\n",
    "        else:\n",
    "            ndmin= find_dims(**kwargs)\n",
    "            WA_mul=assign_wells_multidim(n_dims=ndmin, **kwargs)\n",
    "            multi=['multidim_'+str(ndmin)]\n",
    "            WA_list=[WA_mul]\n",
    "            \n",
    "\n",
    "\n",
    "    else:\n",
    "        ndmin= find_dims(**kwargs)\n",
    "        WA_mul=assign_wells_multidim(n_dims=ndmin, **kwargs)\n",
    "        multi=['multidim: '+str(ndmin)]\n",
    "        WA_list=[WA_mul]\n",
    "    \n",
    "    multi.extend(methods)\n",
    "    methods=multi.copy()\n",
    "\n",
    "    WA_mat=assign_wells_mat(**kwargs)\n",
    "\n",
    "    # random assignment\n",
    "\n",
    "    WA_ran=assign_wells_random(**kwargs)\n",
    "\n",
    "    # STD asignment \n",
    "    WA_std=assign_wells_STD(**kwargs)\n",
    "\n",
    "    \n",
    "\n",
    "    # chinese trick assignment\n",
    "    WA_chin=assign_wells_chinese(**kwargs)\n",
    "\n",
    "\n",
    "    WA_list.extend([WA_mat, WA_ran,WA_std, WA_chin])\n",
    "\n",
    "    WA_bin=assign_wells_L(**kwargs)\n",
    "    methods.append('Binary')\n",
    "    WA_list.append(WA_bin)\n",
    "    this_dir=os.path.join(kwargs['save_dir'],'N_'+str(kwargs['n_compounds']))\n",
    "    if not os.path.exists(this_dir):\n",
    "        os.makedirs(this_dir)\n",
    "\n",
    "    for method, WA in zip(methods, WA_list)\n",
    "\n",
    "\n",
    "\n",
    "    #hierarchical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undiff_WAS(**kwargs):\n",
    "    methods=['matrix', 'random', 'STD', 'Chinese trick']\n",
    "    # matrix assignment\n",
    "    \n",
    "\n",
    "    # multidimensional matrix\n",
    "    if 'n_dims' in kwargs.keys():\n",
    "        WA_mul=assign_wells_multidim(**kwargs)\n",
    "        ndmin=kwargs['n_dims']\n",
    "        multi=['multidim: '+str(ndmin)]\n",
    "        WA_list=[WA_mul]\n",
    "    elif 'all_dims' in kwargs.keys():\n",
    "        if kwargs['all_dims']:\n",
    "            WA_list=[]\n",
    "            multi=[]\n",
    "            for i in np.arange(2,int(np.ceil(np.log(kwargs['n_compounds'])/np.log(2)))):\n",
    "                if i>kwargs['max_dims']:\n",
    "                    continue\n",
    "                WA_mul=assign_wells_multidim(n_dims=i, **kwargs)\n",
    "                WA_list.append(WA_mul)\n",
    "                multi.append('Multidim_'+str(i))\n",
    "\n",
    "        else:\n",
    "            ndmin= find_dims(**kwargs)\n",
    "            WA_mul=assign_wells_multidim(n_dims=ndmin, **kwargs)\n",
    "            multi=['Multidim_'+str(ndmin)]\n",
    "            WA_list=[WA_mul]\n",
    "            \n",
    "\n",
    "\n",
    "    else:\n",
    "        ndmin= find_dims(**kwargs)\n",
    "        WA_mul=assign_wells_multidim(n_dims=ndmin, **kwargs)\n",
    "        multi=['multidim: '+str(ndmin)]\n",
    "        WA_list=[WA_mul]\n",
    "    \n",
    "    multi.append('Matrix')\n",
    "    methods=multi.copy()\n",
    "\n",
    "    WA_mat=assign_wells_mat(**kwargs)\n",
    "\n",
    "\n",
    "    WA_list.append(WA_mat)\n",
    "\n",
    "    if kwargs['differentiate']<2:\n",
    "        WA_bin=assign_wells_L(**kwargs)\n",
    "        methods.append('Binary')\n",
    "        WA_list.append(WA_bin)\n",
    "\n",
    "    #hierarchical\n",
    "        \n",
    "    Hier=calculate_metrics_hierarchical(**kwargs)\n",
    "    #return([BM[0], BM[1],layers, MC, details ])\n",
    "\n",
    "    ls_met=[]\n",
    "    ls_names_met=['mean_experiments', 'max_compounds_per_well', 'n_wells', 'percentage_check', 'mean_extra_exp', 'mean_steps']\n",
    "    for method, WA in zip(methods, WA_list):\n",
    "        mean_exp, extra_exp,  _, perc_check= mean_metrics(WA, **kwargs)\n",
    "        n_wells=WA.shape[1]\n",
    "        M_exp=np.round(mean_exp, 2)\n",
    "        max_comp=np.max(np.sum(WA, axis=0))\n",
    "        ls_met.append([M_exp, max_comp, n_wells, int(perc_check),  extra_exp,1+perc_check/100])\n",
    "    ls_met.append(Hier[:-1])\n",
    "    full_methods=methods.copy()\n",
    "    full_methods.append('Hierarchical')\n",
    "    df_met=pd.DataFrame(ls_met)\n",
    "\n",
    "    dict_wa={method: WA for method, WA in zip(methods, WA_list)}\n",
    "    dict_wa.update({'Hierarchical':Hier[5]})\n",
    "\n",
    "    idx_renamer={i:j for i,j in zip(df_met.index, full_methods)}\n",
    "    col_renamer={i:j for i,j in zip(df_met.columns, ls_names_met)}\n",
    "    df_met.rename(index=idx_renamer, columns=col_renamer, inplace=True)\n",
    "\n",
    "    ret_wa= kwargs['return_wa'] \n",
    "    if ret_wa:\n",
    "        return df_met, dict_wa\n",
    "    return df_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_deterministic_WAS(**kwargs):\n",
    "    methods=['matrix', 'random', 'STD', 'Chinese trick']\n",
    "    # matrix assignment\n",
    "    \n",
    "\n",
    "    # multidimensional matrix\n",
    "    if 'n_dims' in kwargs.keys():\n",
    "        WA_mul=assign_wells_multidim(**kwargs)\n",
    "        ndmin=kwargs['n_dims']\n",
    "        multi=['multidim: '+str(ndmin)]\n",
    "        WA_list=[WA_mul]\n",
    "    elif 'all_dims' in kwargs.keys():\n",
    "        if kwargs['all_dims']:\n",
    "            WA_list=[]\n",
    "            multi=[]\n",
    "            for i in np.arange(2,int(np.ceil(np.log(kwargs['n_compounds'])/np.log(2)))):\n",
    "                if i>kwargs['max_dims']:\n",
    "                    continue\n",
    "                WA_mul=assign_wells_multidim(n_dims=i, **kwargs)\n",
    "                WA_list.append(WA_mul)\n",
    "                multi.append('multidim: '+str(i))\n",
    "\n",
    "        else:\n",
    "            ndmin= find_dims(**kwargs)\n",
    "            WA_mul=assign_wells_multidim(n_dims=ndmin, **kwargs)\n",
    "            multi=['multidim: '+str(ndmin)]\n",
    "            WA_list=[WA_mul]\n",
    "            \n",
    "\n",
    "\n",
    "    else:\n",
    "        ndmin= find_dims(**kwargs)\n",
    "        WA_mul=assign_wells_multidim(n_dims=ndmin, **kwargs)\n",
    "        multi=['multidim: '+str(ndmin)]\n",
    "        WA_list=[WA_mul]\n",
    "    \n",
    "    multi.extend(methods)\n",
    "    methods=multi.copy()\n",
    "\n",
    "    WA_mat=assign_wells_mat(**kwargs)\n",
    "\n",
    "    # random assignment\n",
    "\n",
    "    WA_ran=assign_wells_random(**kwargs)\n",
    "\n",
    "    # STD asignment \n",
    "    WA_std=assign_wells_STD(**kwargs)\n",
    "\n",
    "    \n",
    "\n",
    "    # chinese trick assignment\n",
    "    WA_chin=assign_wells_chinese(**kwargs)\n",
    "\n",
    "\n",
    "    WA_list.extend([WA_mat, WA_ran,WA_std, WA_chin])\n",
    "\n",
    "    if kwargs['differentiate']<2:\n",
    "        WA_bin=assign_wells_L(**kwargs)\n",
    "        methods.append('Binary')\n",
    "        WA_list.append(WA_bin)\n",
    "\n",
    "    #hierarchical\n",
    "        \n",
    "    Hier=calculate_metrics_hierarchical(**kwargs)\n",
    "    #return([BM[0], BM[1],layers, MC, details ])\n",
    "\n",
    "    ls_met=[]\n",
    "    ls_names_met=['mean_experiments', 'max_compounds_per_well', 'n_wells', 'percentage_check', 'mean_extra_exp', 'mean_steps']\n",
    "    for method, WA in zip(methods, WA_list):\n",
    "        mean_exp, extra_exp,  _, perc_check= mean_metrics(WA, **kwargs)\n",
    "        n_wells=WA.shape[1]\n",
    "        M_exp=np.round(mean_exp, 2)\n",
    "        max_comp=np.max(np.sum(WA, axis=0))\n",
    "        ls_met.append([M_exp, max_comp, n_wells, int(perc_check),  extra_exp,1+perc_check/100])\n",
    "    ls_met.append(Hier[:-1])\n",
    "    full_methods=methods.copy()\n",
    "    full_methods.append('Hierarchical')\n",
    "    df_met=pd.DataFrame(ls_met)\n",
    "\n",
    "    dict_wa={method: WA for method, WA in zip(methods, WA_list)}\n",
    "    dict_wa.update({'Hierarchical':Hier[5]})\n",
    "\n",
    "    idx_renamer={i:j for i,j in zip(df_met.index, full_methods)}\n",
    "    col_renamer={i:j for i,j in zip(df_met.columns, ls_names_met)}\n",
    "    df_met.rename(index=idx_renamer, columns=col_renamer, inplace=True)\n",
    "\n",
    "    ret_wa= kwargs['return_wa'] \n",
    "    if ret_wa:\n",
    "        return df_met, dict_wa\n",
    "    return df_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "WA=assign_wells_mat(n_compounds=296)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296, 35)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1],\n",
       "       [  0,   2],\n",
       "       [  0,   3],\n",
       "       ...,\n",
       "       [292, 295],\n",
       "       [293, 295],\n",
       "       [294, 295]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrambler[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, ..., False, False, False],\n",
       "       [ True, False, False, ..., False, False, False],\n",
       "       [ True, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bool(np.sum(WA[scrambler[3]], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " array([[ True, False, False, ..., False, False, False],\n",
       "        [ True, False, False, ..., False, False, False],\n",
       "        [ True, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]]),\n",
       " array([1, 1, 1, ..., 6, 6, 6]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_consistent_precomp(WA,3,scrambler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_metrics_precomp(well_assigner, differentiate, scrambler, **kwargs):\n",
    "    BT=well_assigner.shape[1]\n",
    "    _,_, counts= is_consistent_precomp(well_assigner, differentiate, scrambler) \n",
    "    ET=extra_tests(counts)   \n",
    "    rounds=np.sum(counts>1)/np.sum(counts>0)+1\n",
    "    p_check=np.round(np.sum(counts[counts>1])/np.sum(counts)*100)\n",
    "    return BT+ET, ET,  rounds, p_check\n",
    "\n",
    "def decode_precomp(well_assigner:np.ndarray, readout:np.ndarray, differentiate:int, scrambler:dict) -> list:\n",
    "    N=well_assigner.shape[0]\n",
    "    for i in range(differentiate):\n",
    "        resulti=[]\n",
    "        diff=i+1\n",
    "        if diff ==1:\n",
    "            resulti.extend(list(range(well_assigner.shape[0])))\n",
    "            full_well_assigner=well_assigner.copy()\n",
    "        else:\n",
    "            this_sc=scrambler[diff]\n",
    "            full_well_assigner=np.concatenate((full_well_assigner,np.bool(np.sum(well_assigner[this_sc], axis=1))))\n",
    "    idxs=[i for i in range(full_well_assigner.shape[0]) if np.array_equal(full_well_assigner[i,:],readout)]\n",
    "    if len(idxs)==0:\n",
    "        print('No match')\n",
    "        return(-1)\n",
    "    \n",
    "    return [resulti[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_precomp(well_assigner:np.ndarray, readout:np.ndarray, differentiate:int, scrambler:dict) -> list:\n",
    "    N=well_assigner.shape[0]\n",
    "    for i in range(differentiate):\n",
    "        resulti=[]\n",
    "        diff=i+1\n",
    "        if diff ==1:\n",
    "            resulti.extend(list(range(well_assigner.shape[0])))\n",
    "            full_well_assigner=well_assigner.copy()\n",
    "        else:\n",
    "            this_sc=scrambler[diff]\n",
    "            full_well_assigner=np.concatenate((full_well_assigner,np.bool(np.sum(well_assigner[this_sc], axis=1))))\n",
    "    idxs=[i for i in range(full_well_assigner.shape[0]) if np.array_equal(full_well_assigner[i,:],readout)]\n",
    "    if len(idxs)==0:\n",
    "        print('No match')\n",
    "        return(-1)\n",
    "    \n",
    "    return [resulti[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(39.79847019272499),\n",
       " np.float64(4.798470192724995),\n",
       " np.float64(1.956555662663945),\n",
       " np.float64(99.0))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metrics_precomp(WA,3,scrambler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_deterministic_comparison(**kwargs):\n",
    "    methods=['matrix', 'random', 'STD', 'Chinese trick']\n",
    "    # matrix assignment\n",
    "    \n",
    "\n",
    "    # multidimensional matrix\n",
    "    if 'n_dims' in kwargs.keys():\n",
    "        WA_mul=assign_wells_multidim(**kwargs)\n",
    "        ndmin=kwargs['n_dims']\n",
    "        multi=['multidim: '+str(ndmin)]\n",
    "        WA_list=[WA_mul]\n",
    "    elif 'all_dims' in kwargs.keys():\n",
    "        if kwargs['all_dims']:\n",
    "            WA_list=[]\n",
    "            multi=[]\n",
    "            for i in np.arange(2,int(np.ceil(np.log(kwargs['n_compounds'])/np.log(2)))):\n",
    "                if i>kwargs['max_dims']:\n",
    "                    continue\n",
    "                WA_mul=assign_wells_multidim(n_dims=i, **kwargs)\n",
    "                WA_list.append(WA_mul)\n",
    "                multi.append('multidim: '+str(i))\n",
    "\n",
    "        else:\n",
    "            ndmin= find_dims(**kwargs)\n",
    "            WA_mul=assign_wells_multidim(n_dims=ndmin, **kwargs)\n",
    "            multi=['multidim: '+str(ndmin)]\n",
    "            WA_list=[WA_mul]\n",
    "            \n",
    "\n",
    "\n",
    "    else:\n",
    "        ndmin= find_dims(**kwargs)\n",
    "        WA_mul=assign_wells_multidim(n_dims=ndmin, **kwargs)\n",
    "        multi=['multidim: '+str(ndmin)]\n",
    "        WA_list=[WA_mul]\n",
    "    \n",
    "    multi.extend(methods)\n",
    "    methods=multi.copy()\n",
    "\n",
    "    WA_mat=assign_wells_mat(**kwargs)\n",
    "\n",
    "    # random assignment\n",
    "\n",
    "    WA_ran=assign_wells_random(**kwargs)\n",
    "\n",
    "    # STD asignment \n",
    "    WA_std=assign_wells_STD(**kwargs)\n",
    "\n",
    "    \n",
    "\n",
    "    # chinese trick assignment\n",
    "    WA_chin=assign_wells_chinese(**kwargs)\n",
    "\n",
    "\n",
    "    WA_list.extend([WA_mat, WA_ran,WA_std, WA_chin])\n",
    "\n",
    "    if kwargs['differentiate']<2:\n",
    "        WA_bin=assign_wells_L(**kwargs)\n",
    "        methods.append('Binary')\n",
    "        WA_list.append(WA_bin)\n",
    "\n",
    "    #hierarchical\n",
    "        \n",
    "    Hier=calculate_metrics_hierarchical(**kwargs)\n",
    "    #return([BM[0], BM[1],layers, MC, details ])\n",
    "\n",
    "    ls_met=[]\n",
    "    ls_names_met=['mean_experiments', 'max_compounds_per_well', 'n_wells', 'percentage_check', 'mean_extra_exp', 'mean_steps']\n",
    "    for method, WA in zip(methods, WA_list):\n",
    "        mean_exp, extra_exp,  _, perc_check= mean_metrics(WA, **kwargs)\n",
    "        n_wells=WA.shape[1]\n",
    "        M_exp=np.round(mean_exp, 2)\n",
    "        max_comp=np.max(np.sum(WA, axis=0))\n",
    "        ls_met.append([M_exp, max_comp, n_wells, int(perc_check),  extra_exp,1+perc_check/100])\n",
    "    ls_met.append(Hier[:-1])\n",
    "    full_methods=methods.copy()\n",
    "    full_methods.append('Hierarchical')\n",
    "    df_met=pd.DataFrame(ls_met)\n",
    "\n",
    "    dict_wa={method: WA for method, WA in zip(methods, WA_list)}\n",
    "    dict_wa.update({'Hierarchical':Hier[5]})\n",
    "\n",
    "    idx_renamer={i:j for i,j in zip(df_met.index, full_methods)}\n",
    "    col_renamer={i:j for i,j in zip(df_met.columns, ls_names_met)}\n",
    "    df_met.rename(index=idx_renamer, columns=col_renamer, inplace=True)\n",
    "\n",
    "    ret_wa= kwargs['return_wa'] \n",
    "    if ret_wa:\n",
    "        return df_met, dict_wa\n",
    "    return df_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./combinations/N_294.pk', 'rb') as handle:\n",
    "    f1=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F2=iterative_add_N(f1,400, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1=add_N(F_dict,6,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cmb_dict.pk', 'wb') as handle:\n",
    "    pickle.dump(F1, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1[1000][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(itertools.combinations(np.arange(1000),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri=add_1(dct_cmbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(itertools.combinations(np.arange(6),2))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_dir='./diff_1/20-47_step5.pk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_dir='./single_method/multidim_3/multidim_3__20-47_step5.pk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pk_dir, 'rb') as handle:\n",
    "    f1=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1[45]['WA'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To merge .pk files\n",
    "\n",
    "ls_names_met=['mean_experiments', 'max_compounds_per_well', 'n_wells', 'percentage_check', 'mean_extra_exp']\n",
    "full_dict={}\n",
    "for j in range(1,5):\n",
    "    base_dir='diff_'+str(j)\n",
    "    diff_dict={}\n",
    "    filenames = next(os.walk(base_dir), (None, None, []))[2]\n",
    "    for file in filenames:\n",
    "        full_dir=os.path.join(base_dir,file)\n",
    "        with open(full_dir, 'rb') as handle:\n",
    "            f1=pickle.load(handle)\n",
    "        for i in list(f1):\n",
    "            if i=='kwargs':\n",
    "                del f1['kwargs']\n",
    "                continue\n",
    "            else:\n",
    "                WA=assign_wells_mat(i)\n",
    "                f1[i][1].update({'matrix':WA})\n",
    "                mean_exp, extra_exp,  _, perc_check= mean_metrics(WA, differentiate=j)\n",
    "                n_wells=WA.shape[1]\n",
    "                M_exp=np.round(mean_exp, 2)\n",
    "                max_comp=np.max(np.sum(WA, axis=0))\n",
    "                ls_met=[M_exp, max_comp, n_wells, perc_check,  extra_exp]\n",
    "                f2=f1[i][0].drop(labels='matrix')\n",
    "                idxs=list(f2.index)\n",
    "                idxs+=['matrix']\n",
    "                r_idxs=idxs[-1:]+idxs[:-1]\n",
    "                dict_met={nm:list(f2[nm])+[val] for nm,val in zip(ls_names_met, ls_met)}\n",
    "\n",
    "                f1[i][0]=pd.DataFrame(data=dict_met, index=idxs).reindex(r_idxs)\n",
    "\n",
    "                \n",
    "                f1[i][0]=f1[i][0].round({'mean_extra_exp':2})\n",
    "            \n",
    "        diff_dict.update(f1)\n",
    "    full_str='Differentiate '+str(j)\n",
    "    full_dict.update({full_str:diff_dict})\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output as pickle file\n",
    "full_dir='Final_precomputed_file.pk'\n",
    "\n",
    "#with open(full_dir, 'wb') as handle:\n",
    "    #pickle.dump(full_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To merge files generated with inline_print\n",
    "all_methods = ['matrix', 'multidim', 'random', 'STD', 'CT', 'Binary']\n",
    "results = {}\n",
    "\n",
    "for method_dir in all_methods:\n",
    "    filenames = next(os.walk(method_dir), (None, None, []))[2]\n",
    "    metrics_list = []\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".txt\"):\n",
    "            try:\n",
    "                metrics = extract_metrics(file)\n",
    "                metrics[\"method\"] = method_dir\n",
    "                metrics_list.append(metrics)\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "    \n",
    "    if metrics_list:\n",
    "        df = pd.DataFrame(metrics_list)\n",
    "        df = df.sort_values(by=\"NS\")\n",
    "        \n",
    "        # Group by diff value\n",
    "        for diff_value, group_df in df.groupby(\"diff\"):\n",
    "            group_df = group_df.reset_index(drop=True)\n",
    "            \n",
    "            if diff_value not in results:\n",
    "                results['Differentiate ' + str(diff_value)] = {}\n",
    "                \n",
    "            for ns_value, ns_group_df in group_df.groupby(\"NS\"):\n",
    "                ns_group_df = ns_group_df.drop(columns=[\"diff\", \"NS\"]) \n",
    "                ns_group_df = ns_group_df.rename(columns={\"NW\": \"n_wells\", \"MS\": \"max_compounds_per_well\"})\n",
    "               \n",
    "                ns_group_df = ns_group_df.set_index([\"method\"])\n",
    "                \n",
    "                if ns_value not in results['Differentiate ' + str(diff_value)]:\n",
    "                    results['Differentiate ' + str(diff_value)][ns_value] = [ns_group_df, {}]\n",
    "                else:\n",
    "                    existing_df, empty_dict = results['Differentiate ' + str(diff_value)][ns_value]\n",
    "                    \n",
    "                    merged_df = pd.merge(existing_df.reset_index(), ns_group_df.reset_index(),\n",
    "                                         on=[\"method\", \"n_wells\", \"max_compounds_per_well\"], how=\"outer\", indicator=True)\n",
    "                    \n",
    "                    # Filter out rows that are duplicates (the '_merge' column indicates if it's from both dataframes)\n",
    "                    new_entries_df = merged_df[merged_df['_merge'] == 'right_only'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    existing_entries_df = merged_df[merged_df['_merge'] == 'both'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    \n",
    "                    # Concatenate only new entries\n",
    "                    combined_df = pd.concat([existing_entries_df, new_entries_df]).drop_duplicates().reset_index(drop=True)\n",
    "                    \n",
    "                    # Reassign the combined dataframe\n",
    "                    results['Differentiate ' + str(diff_value)][ns_value] = [combined_df, {}]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Output as pickle file\n",
    "full_dir = 'Final_inline_precomputed_file.pk'\n",
    "\n",
    "# First check if file already exists\n",
    "# Merge new and old if it does\n",
    "if os.path.exists(full_dir):\n",
    "    with open(full_dir, \"rb\") as f:\n",
    "        existing_results = pickle.load(f)\n",
    "        \n",
    "    for diff_value, ns_values in results.items():\n",
    "        if diff_value not in existing_results:\n",
    "            existing_df, existing_empty_dict = existing_results[diff_value][ns_value]\n",
    "            \n",
    "            for ns_value, (new_df, empty_dict) in ns_values.items():\n",
    "                if ns_value in existing_results[diff_value]:\n",
    "                    existing_df, existing_empty_dict = existing_results[diff_value][ns_value]\n",
    "                \n",
    "            \n",
    "                    merged_df = pd.merge(existing_df.reset_index(), new_df.reset_index(),\n",
    "                                         on=[\"method\", \"n_wells\", \"max_compounds_per_well\"], how=\"outer\", indicator=True)\n",
    "                    \n",
    "                    new_entries_df = merged_df[merged_df['_merge'] == 'right_only'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    existing_entries_df = merged_df[merged_df['_merge'] == 'both'].drop(columns=[\"_merge\"]).set_index(\"method\")\n",
    "                    \n",
    "                    combined_df = pd.concat([existing_entries_df, new_entries_df]).drop_duplicates().reset_index(drop=True)\n",
    "                    existing_results[diff_value][ns_value] = [combined_df.set_index([\"method\"]), {}]\n",
    "                else:\n",
    "                    existing_results[diff_value][ns_value] = [new_df, empty_dict]\n",
    "            \n",
    "                \n",
    "    merged_results = existing_results\n",
    "    \n",
    "else:\n",
    "    merged_results = results\n",
    "\n",
    "#with open(full_dir, 'wb') as handle:\n",
    "    #pickle.dump(merged_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pooling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
